/* This code and any associated documentation is provided "as is"

Copyright 2025 Munich Quantum Software Stack Project

Licensed under the Apache License, Version 2.0 with LLVM Exceptions (the
"License"); you may not use this file except in compliance with the License.
You may obtain a copy of the License at

https://github.com/Munich-Quantum-Software-Stack/passes/blob/develop/LICENSE

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
License for the specific language governing permissions and limitations under
the License.

SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
-------------------------------------------------------------------------
  author Martin Letras
  date   July 2025
  version 1.0
  brief
    Definition of the translation from Quake to LinAlg + Artih. The intention
  of converting to LinAlg + Artih is, later be able to use IREE compiler to
  generate code to many different GPUs vendors. As far as we know the LinAlg
  + Arith is a representation supported by IREE.
  TODO: At the moment the matrices generated by each gate are defined as private
  and empty. The definition of the matrices has to be declared later during
  the compilation flow.
    E.g., using a 3 qubits system, the vector space is 2^3. And each matrix
    representing the gates are of size [2^3 x 2^3]
    When translating:
    module {
      func.func @quake() {
        %0 = quake.alloca !quake.veq<3>
        %1 = quake.extract_ref %0[0] : (!quake.veq<3>) -> !quake.ref
        quake.h %1 : (!quake.ref) -> ()
        return
      }
    }
    The equivalent in LinAlg + Artih looks as follows:
    module {
      // generated function that returns a matrix [2^3 x 2^3].
      //This is the type of function that has to be defined later func.func
      func.func private @"h_3qubits_control[]_target[0]"() ->
tensor<8x8xcomplex<f64>> func.func @quake() { %cst = arith.constant
dense<(0.000000e+00,0.000000e+00)> : tensor<8xcomplex<f64>> %0 = call
@"h_3qubits_control[]_target[0]"() : () -> tensor<8x8xcomplex<f64> %1 =
tensor.empty() : tensor<8xcomplex<f64>> %2 = linalg.matvec ins(%0, %cst :
tensor<8x8xcomplex<f64>>, tensor<8xcomplex<f64>>) outs(%1 :
tensor<8xcomplex<f64>>) -> tensor<8xcomplex<f64>> return %2
      }
    }

*******************************************************************************
* This source code and the accompanying materials are made available under    *
* the terms of the Apache License 2.0 which accompanies this distribution.    *
******************************************************************************/

#include "Interfaces/QuakeToLinAlg.hpp"
#include "Support/CodeGen/Quake.hpp"
#include "cudaq/Optimizer/Dialect/CC/CCOps.h"
#include "cudaq/Optimizer/Dialect/CC/CCTypes.h"
#include "cudaq/Support/Plugin.h"
#include "mlir/Dialect/Complex/IR/Complex.h"
#include "mlir/Dialect/Func/IR/FuncOps.h"
#include "mlir/Dialect/Linalg/IR/Linalg.h"
#include "mlir/Dialect/SCF/IR/SCF.h"
#include "mlir/Dialect/Tensor/IR/Tensor.h"
#include "mlir/Rewrite/FrozenRewritePatternSet.h"
#include "mlir/Transforms/DialectConversion.h"

#include "llvm/Support/Casting.h"
#include "llvm/Support/raw_ostream.h"

#include <complex>
#include <iostream>

using namespace mlir;
using namespace mlir::complex;
using namespace mlir::utils;
using namespace mqss::support::quakeDialect;

bool hasFunc(mlir::ModuleOp module, llvm::StringRef name) {
  return static_cast<bool>(module.lookupSymbol<mlir::func::FuncOp>(name));
}

mlir::func::FuncOp getFuncOp(mlir::ModuleOp module, llvm::StringRef name) {
  return module.lookupSymbol<mlir::func::FuncOp>(name);
}

/// Two‑operand multiply helper.
/// ops[0] and ops[1] must be the same element type (complex<f32|f64>)
/// and ranks among {0,1,2}. Returns a Value of the multiplied result.
Value insertMul(OpBuilder &builder, Location loc, ArrayRef<Value> ops) {
  assert(ops.size() == 2 && "insertMul requires exactly two operands");

  // Fetch the tensor type for operand 0 (we assume both have same shape/type).
  auto rt0 = ops[0].getType().dyn_cast<RankedTensorType>();
  auto rt1 = ops[1].getType().dyn_cast<RankedTensorType>();
  assert(rt0 && rt1 && "Operands must be RankedTensorType");

  auto rank0 = rt0.getRank();
  auto rank1 = rt1.getRank();
  auto eltType = rt0.getElementType().dyn_cast<ComplexType>();
  assert(eltType && eltType == rt1.getElementType().dyn_cast<ComplexType>() &&
         "Element types must match and be complex");

  // --- Case A: Scalar (rank0 == 0 && rank1 == 0) ---
  if (rank0 == 0 && rank1 == 0) {
    return builder.create<mlir::complex::MulOp>(loc, eltType, ops[0], ops[1])
        .getResult();
  }

  // --- Case B: Vector (rank0 == 1 && rank1 == 1, same shape) ---
  if (rank0 == 1 && rank1 == 1 && rt0.getShape() == rt1.getShape()) {
    // Create empty output tensor
    Value resultTensor =
        builder.create<mlir::tensor::EmptyOp>(loc, rt0.getShape(), eltType);

    auto ctx = builder.getContext();
    auto dimExpr = mlir::getAffineDimExpr(0, ctx);
    auto map =
        mlir::AffineMap::get(/*dimCount=*/1, /*symbolCount=*/0, {dimExpr});
    SmallVector<mlir::AffineMap> indexingMaps = {map, map, map};
    SmallVector<mlir::utils::IteratorType> iterTypes = {
        mlir::utils::IteratorType::parallel};

    auto genericOp = builder.create<mlir::linalg::GenericOp>(
        loc, TypeRange{rt0}, ValueRange{ops[0], ops[1]},
        ValueRange{resultTensor}, indexingMaps, iterTypes,
        /*doc=*/"",
        /*libraryCall=*/"",
        [&](OpBuilder &nestedBuilder, Location nestedLoc, ValueRange args) {
          Value prod = nestedBuilder.create<mlir::complex::MulOp>(
              nestedLoc, eltType, args[0], args[1]);
          nestedBuilder.create<mlir::linalg::YieldOp>(nestedLoc, prod);
        });

    return genericOp.getResult(0);
  }

  // --- Case C: Matrix × Vector → Matvec (rank0==2, rank1==1) ---
  if (rank0 == 2 && rank1 == 1 && rt0.getDimSize(1) == rt1.getDimSize(0)) {
    // Init result tensor< M x complex >
    int64_t M = rt0.getDimSize(0);
    auto resultType = RankedTensorType::get({M}, eltType);
    Value init = builder.create<mlir::tensor::EmptyOp>(
        loc, ArrayRef<int64_t>{M}, eltType);
    return builder
        .create<linalg::MatvecOp>(loc, resultType, ValueRange{ops[0], ops[1]},
                                  ValueRange{init})
        .getResult(0);
  }

  // --- Case D: Matrix × Matrix → Matmul (rank0==2, rank1==2) ---
  if (rank0 == 2 && rank1 == 2 && rt0.getDimSize(1) == rt1.getDimSize(0)) {
    int64_t M = rt0.getDimSize(0), K = rt0.getDimSize(1), N = rt1.getDimSize(1);
    auto resultType = RankedTensorType::get({M, N}, eltType);
    Value init = builder.create<mlir::tensor::EmptyOp>(
        loc, ArrayRef<int64_t>{M, N}, eltType);
    return builder
        .create<linalg::MatmulOp>(loc, resultType, ValueRange{ops[0], ops[1]},
                                  ValueRange{init})
        .getResult(0);
  }

  llvm_unreachable("Unsupported operand shapes for insertMul");
}

/// Insert an N‑ary multiply by folding the 2‑operand case.
/// Requires at least 2 operands.
Value insertMulN(OpBuilder &builder, Location loc, ArrayRef<Value> ops) {
  assert(ops.size() >= 2 && "Need at least two operands to multiply");

  // Start by multiplying the first two:
  SmallVector<Value, 4> two = {ops[0], ops[1]};
  Value acc = insertMul(builder, loc, two);

  // Then fold in the rest
  for (size_t i = 2, e = ops.size(); i < e; ++i) {
    two[0] = acc;
    two[1] = ops[i];
    acc = insertMul(builder, loc, two);
  }

  return acc;
}

mlir::Value initializeQubits(mlir::func::FuncOp gpuFunction,
                             mlir::OpBuilder &builder, int numberOfQubits,
                             mlir::RankedTensorType tensorType) {
  mlir::Block &entryBlock = gpuFunction.getBody().front();
  builder.setInsertionPointToStart(&entryBlock);

  std::vector<std::complex<double>> data;
  for (int i = 0; i < std::pow(2, numberOfQubits); i++)
    data.push_back({0.0, 0.0});

  mlir::DenseElementsAttr initAttr =
      mlir::DenseElementsAttr::get(tensorType, llvm::makeArrayRef(data));
  mlir::Location loc = builder.getUnknownLoc();

  auto constantOp =
      builder.create<mlir::arith::ConstantOp>(loc, tensorType, initAttr);
  return constantOp;
}

mlir::func::FuncOp inlineFunction(mlir::ModuleOp &module,
                                  std::string operationName,
                                  mlir::RankedTensorType matrixType,
                                  size_t numberParameters) {
  // if the function is already in the module,
  // then return it
  if (hasFunc(module, operationName))
    return getFuncOp(module, operationName);
  mlir::OpBuilder builder(module.getBodyRegion());
  mlir::Location loc = builder.getUnknownLoc();
  // if the gate has parameters the signature function has to
  // accept a vector of size of the parameters
  mlir::FunctionType funcType;
  if (numberParameters > 0) {
    auto elementType = builder.getF64Type();
    mlir::RankedTensorType tensorType =
        mlir::RankedTensorType::get({numberParameters}, elementType);
    funcType = builder.getFunctionType({tensorType}, {matrixType});
  } else {
    funcType = builder.getFunctionType({}, {matrixType});
  }

  auto operationFunction =
      builder.create<mlir::func::FuncOp>(loc, operationName, funcType);
  operationFunction.setPrivate(); // Optional: make it private visibility
  operationFunction.setVisibility(mlir::SymbolTable::Visibility::Private);
  // Erase the body to make it external (opaque)
  operationFunction.eraseBody();
  return operationFunction;
}

mlir::Value createDoubleTensor(mlir::OpBuilder &builder, mlir::Location loc,
                               const std::vector<double> &dataVec) {
  // Create the tensor type: tensor<Nxf64>
  auto elementType = builder.getF64Type();
  auto tensorType = mlir::RankedTensorType::get(
      {static_cast<int64_t>(dataVec.size())}, elementType);

  // Convert to APFloat and build a DenseElementsAttr
  llvm::SmallVector<mlir::APFloat, 4> apValues;
  for (double val : dataVec)
    apValues.emplace_back(mlir::APFloat(val));

  auto attr = mlir::DenseElementsAttr::get(tensorType, apValues);

  // Create the constant operation
  return builder.create<mlir::arith::ConstantOp>(loc, tensorType, attr)
      .getResult();
}

mlir::Value mqss::interfaces::convertQuakeToLinAlg(
    mlir::ModuleOp module, mlir::func::FuncOp quakeFunction, OpBuilder &builder,
    func::FuncOp gpuFunction, mlir::RankedTensorType tensorType,
    mlir::RankedTensorType matrixType, int numberOfQubits) {
  // insert the initial state of the circuit
  Value state =
      initializeQubits(gpuFunction, builder, numberOfQubits, tensorType);
  // iterate the function to see if it is quake
  quakeFunction.walk([&](mlir::Operation *op) {
    auto gate = dyn_cast<quake::OperatorInterface>(op);
    if (!gate)
      return;
    // then, the operation is a quake gate
    llvm::StringRef opName = op->getName().getStringRef();
    std::regex pattern("^quake\\.");
    std::string gateName = std::regex_replace(opName.str(), pattern, "");
    // get the target and get the controls
    std::vector<int> controls = getIndicesOfValueRange(gate.getControls());
    std::vector<int> targets = getIndicesOfValueRange(gate.getTargets());
    std::vector<double> params = getParametersValues(gate.getParameters());
    std::string controlString = "[";
    std::string targetString = "[";
    for (int i = 0; i < controls.size(); i++)
      controlString += std::to_string(controls[i]);
    controlString += "]";
    for (int i = 0; i < targets.size(); i++)
      targetString += std::to_string(targets[i]);
    targetString += "]";
    std::string functionName = gateName + "_" + std::to_string(numberOfQubits) +
                               "qubits_" + "control" + controlString +
                               "_target" + targetString;
    // inline the function
    mlir::Location loc = builder.getUnknownLoc();
    mlir::func::FuncOp operationFunction =
        inlineFunction(module, functionName, matrixType, params.size());
    // Emit func.call to the private function
    mlir::Value matrixValue;
    if (params.size() > 0) {
      mlir::Value arguments = createDoubleTensor(builder, loc, params);
      matrixValue =
          builder.create<func::CallOp>(loc, functionName, matrixType, arguments)
              .getResult(0);
    } else {
      matrixValue =
          builder
              .create<func::CallOp>(loc, functionName, matrixType, ValueRange{})
              .getResult(0);
    }
    SmallVector<mlir::Value> operands;
    operands.push_back(matrixValue);
    operands.push_back(state);
    state = insertMulN(builder, loc, operands);
  });
  return state;
}
